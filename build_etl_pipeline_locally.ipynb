{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing GlueContext\n",
    "reference: https://aws.amazon.com/blogs/big-data/building-an-aws-glue-etl-pipeline-locally-without-an-aws-account/\n",
    "\n",
    "To get started, enter the following import statements in the PySpark shell. We import GlueContext, which wraps the Spark SQLContext, thereby providing mechanisms to interact with Apache Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.transforms import *\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "glueContext = GlueContext(SparkContext.getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 1\n",
    "We first generate a Spark DataFrame consisting of dummy data of an order list for a fictional company. We process the data using AWS Glue PySpark functions.\n",
    "\n",
    "Enter the following code into the shell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_list = [\n",
    "               ['1005', '623', 'YES', '1418901234', '75091'],\\\n",
    "               ['1006', '547', 'NO', '1418901256', '75034'],\\\n",
    "               ['1007', '823', 'YES', '1418901300', '75023'],\\\n",
    "               ['1008', '912', 'NO', '1418901400', '82091'],\\\n",
    "               ['1009', '321', 'YES', '1418902000', '90093']\\\n",
    "             ]\n",
    "\n",
    "# Define schema for the order_list\n",
    "order_schema = StructType([  \n",
    "                      StructField(\"order_id\", StringType()),\n",
    "                      StructField(\"customer_id\", StringType()),\n",
    "                      StructField(\"essential_item\", StringType()),\n",
    "                      StructField(\"timestamp\", StringType()),\n",
    "                      StructField(\"zipcode\", StringType())\n",
    "                    ])\n",
    "\n",
    "# Create a Spark Dataframe from the python list and the schema\n",
    "df_orders = spark.createDataFrame(order_list, schema = order_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DynamicFrame\n",
    "A DynamicFrame is similar to a DataFrame, except that each record is self-describing, so no schema is required initially. Instead, AWS Glue computes a schema on-the-fly when required. We convert the df_orders DataFrame into a DynamicFrame.\n",
    "\n",
    "Enter the following code in the shell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyf_orders = DynamicFrame.fromDF(df_orders, glueContext, \"dyf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our Dynamic Frame, we can start working with the datasets with AWS Glue transform functions.\n",
    "\n",
    "## ApplyMapping\n",
    "The columns in our data might be in different formats, and you may want to change their respective names. ApplyMapping is the best option for changing the names and formatting all the columns collectively. For our dataset, we change some of the columns to Long from String format to save storage space later. We also shorten the column zipcode to zip. See the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input \n",
    "dyf_applyMapping = ApplyMapping.apply( frame = dyf_orders, mappings = [ \n",
    "  (\"order_id\",\"String\",\"order_id\",\"Long\"), \n",
    "  (\"customer_id\",\"String\",\"customer_id\",\"Long\"),\n",
    "  (\"essential_item\",\"String\",\"essential_item\",\"String\"),\n",
    "  (\"timestamp\",\"String\",\"timestamp\",\"Long\"),\n",
    "  (\"zipcode\",\"String\",\"zip\",\"Long\")\n",
    "])\n",
    "\n",
    "dyf_applyMapping.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter\n",
    "We now want to prioritize our order delivery for essential items. We can achieve that using the Filter function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input \n",
    "dyf_filter = Filter.apply(frame = dyf_applyMapping, f = lambda x: x[\"essential_item\"] == 'YES')\n",
    "\n",
    "dyf_filter.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map\n",
    "Map allows us to apply a transformation to each record of a Dynamic Frame. For our case, we want to target a certain zip code for next day air shipping. We implement a simple “next_day_air” function and pass it to the Dynamic Frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input \n",
    "\n",
    "# This function takes in a dynamic frame record and checks if zipcode # 75034 is present in it. If present, it adds another column \n",
    "# “next_day_air” with value as True\n",
    "\n",
    "def next_day_air(rec):\n",
    "  if rec[\"zip\"] == 75034:\n",
    "    rec[\"next_day_air\"] = True\n",
    "  return rec\n",
    "\n",
    "mapped_dyF =  Map.apply(frame = dyf_applyMapping, f = next_day_air)\n",
    "\n",
    "mapped_dyF.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 2\n",
    "To ship essential orders to the appropriate addresses, we need customer data. We demonstrate this by generating a custom JSON dataset consisting of zip codes and customer addresses. In this use case, this data represents the customer data of the company that we want to join later on.\n",
    "\n",
    "We generate JSON strings consisting of customer data and use the Spark json function to convert them to a JSON structure (enter each jsonStr variable one at a time in case the terminal errors out):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input \n",
    "jsonStr1 = u'{ \"zip\": 75091, \"customers\": [{ \"id\": 623, \"address\": \"108 Park Street, TX\"}, { \"id\": 231, \"address\": \"763 Marsh Ln, TX\" }]}'\n",
    "jsonStr2 = u'{ \"zip\": 82091, \"customers\": [{ \"id\": 201, \"address\": \"771 Peek Pkwy, GA\" }]}'\n",
    "jsonStr3 = u'{ \"zip\": 75023, \"customers\": [{ \"id\": 343, \"address\": \"66 P Street, NY\" }]}'\n",
    "jsonStr4 = u'{ \"zip\": 90093, \"customers\": [{ \"id\": 932, \"address\": \"708 Fed Ln, CA\"}, { \"id\": 102, \"address\": \"807 Deccan Dr, CA\" }]}'\n",
    "df_row = spark.createDataFrame([\n",
    "  Row(json=jsonStr1),\n",
    "  Row(json=jsonStr2),\n",
    "  Row(json=jsonStr3),\n",
    "  Row(json=jsonStr4)\n",
    "])\n",
    "\n",
    "df_json = spark.read.json(df_row.rdd.map(lambda r: r.json))\n",
    "df_json.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "df_json.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert the DataFrame back to a DynamicFrame to continue with our operations, enter the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "dyf_json = DynamicFrame.fromDF(df_json, glueContext, \"dyf_json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SelectFields\n",
    "To join with the order list, we don’t need all the columns, so we use the SelectFields function to shortlist the columns we need. In our use case, we need the zip code column, but we can add more columns as the argument paths accepts a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "dyf_selectFields = SelectFields.apply(frame = dyf_filter, paths=['zip'])\n",
    "\n",
    "dyf_selectFields.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join\n",
    "The Join function is straightforward and manages duplicate columns. We had two columns named zip from both datasets. AWS Glue added a period (.) in one of the duplicate column names to avoid errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "dyf_join = Join.apply(dyf_json, dyf_selectFields, 'zip', 'zip')\n",
    "dyf_join.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DropFields\n",
    "Because we don’t need two columns with the same name, we can use DropFields to drop one or multiple columns all at once. The backticks (`) around .zip inside the function call are needed because the column name contains a period (.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "dyf_dropfields = DropFields.apply(\n",
    "  frame = dyf_join,\n",
    "  paths = \"`.zip`\"\n",
    ")\n",
    "\n",
    "dyf_dropfields.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationalize\n",
    "The Relationalize function can flatten nested structures and create multiple dynamic frames. Our customer column from the previous operation is a nested structure, and Relationalize can convert it into multiple flattened DynamicFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "dyf_relationize = dyf_dropfields.relationalize(\"root\", \"/home/glue/GlueLocalOutput\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the DynamicFrames, we can’t run a .show() yet because it’s a collection. We need to check what keys are present. See the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "dyf_relationize.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the follow-up function in the next section, we show how to pick the DynamicFrame from a collection of multiple DynamicFrames.\n",
    "\n",
    "## SelectFromCollection\n",
    "The SelectFromCollection function allows us to retrieve the specific DynamicFrame from a collection of DynamicFrames. For this use case, we retrieve both DynamicFrames from the previous operation using this function.\n",
    "\n",
    "To retrieve the first DynamicFrame, enter the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "dyf_selectFromCollection = SelectFromCollection.apply(dyf_relationize, 'root')\n",
    "\n",
    "dyf_selectFromCollection.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrieve the second DynamicFrame, enter the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "dyf_selectFromCollection = SelectFromCollection.apply(dyf_relationize, 'root_customers')\n",
    "\n",
    "dyf_selectFromCollection.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RenameField\n",
    "The second DynamicFrame we retrieved from the previous operation introduces a period (.) into our column names and is very lengthy. We can change that using the RenameField function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "dyf_renameField_1 = RenameField.apply(dyf_selectFromCollection, \"`customers.val.address`\", \"address\")\n",
    "\n",
    "dyf_renameField_2 = RenameField.apply(dyf_renameField_1, \"`customers.val.id`\", \"cust_id\")\n",
    "\n",
    "dyf_dropfields_rf = DropFields.apply(\n",
    "  frame = dyf_renameField_2,\n",
    "  paths = [\"index\", \"id\"]\n",
    ")\n",
    "\n",
    "dyf_dropfields_rf.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResolveChoice\n",
    "ResloveChoice can gracefully handle column type ambiguities. For more information about the full capabilities of ResolveChoice, see the GitHub repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "dyf_resolveChoice = dyf_dropfields_rf.resolveChoice(specs = [('cust_id','cast:String')])\n",
    "\n",
    "dyf_resolveChoice.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 3\n",
    "We generate another dataset to demonstrate a few other functions. In this use case, the company’s warehouse inventory data is in a nested JSON structure, which is initially in a String format. See the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "warehouse_inventory_list = [\n",
    "              ['TX_WAREHOUSE', '{\\\n",
    "                          \"strawberry\":\"220\",\\\n",
    "                          \"pineapple\":\"560\",\\\n",
    "                          \"mango\":\"350\",\\\n",
    "                          \"pears\":null}'\n",
    "               ],\\\n",
    "              ['CA_WAREHOUSE', '{\\\n",
    "                         \"strawberry\":\"34\",\\\n",
    "                         \"pineapple\":\"123\",\\\n",
    "                         \"mango\":\"42\",\\\n",
    "                         \"pears\":null}\\\n",
    "              '],\n",
    "    \t\t   ['CO_WAREHOUSE', '{\\\n",
    "                         \"strawberry\":\"340\",\\\n",
    "                         \"pineapple\":\"180\",\\\n",
    "                         \"mango\":\"2\",\\\n",
    "                         \"pears\":null}'\n",
    "              ]\n",
    "            ]\n",
    "\n",
    "\n",
    "warehouse_schema = StructType([StructField(\"warehouse_loc\", StringType())\\\n",
    "                              ,StructField(\"data\", StringType())])\n",
    "\n",
    "df_warehouse = spark.createDataFrame(warehouse_inventory_list, schema = warehouse_schema)\n",
    "dyf_warehouse = DynamicFrame.fromDF(df_warehouse, glueContext, \"dyf_warehouse\")\n",
    "\n",
    "dyf_warehouse.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unbox\n",
    "We use Unbox to extract JSON from String format for the new data. Compare the preceding printSchema() output with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "dyf_unbox = Unbox.apply(frame = dyf_warehouse, path = \"data\", format=\"json\")\n",
    "dyf_unbox.printSchema()\n",
    "\n",
    "# Input \n",
    "dyf_unbox.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unnest\n",
    "Unnest allows us to flatten a single DynamicFrame to a more relational table format. We apply Unnest to the nested structure from the previous operation and flatten it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "dyf_unnest = UnnestFrame.apply(frame = dyf_unbox)\n",
    "\n",
    "dyf_unnest.printSchema()\n",
    "\n",
    "dyf_unnest.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DropNullFields\n",
    "The DropNullFields function makes it easy to drop columns with all null values. Our warehouse data indicated that it was out of pears and can be dropped. We apply the DropNullFields function on the DynamicFrame, which automatically identifies the columns with null values and drops them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "dyf_dropNullfields = DropNullFields.apply(frame = dyf_unnest)\n",
    "\n",
    "dyf_dropNullfields.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SplitFields\n",
    "SplitFields allows us to split a DyanmicFrame into two. The function takes the field names of the first DynamicFrame that we want to generate followed by the names of the two DynamicFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "dyf_splitFields = SplitFields.apply(frame = dyf_dropNullfields, paths = [\"`data.strawberry`\", \"`data.pineapple`\"], name1 = \"a\", name2 = \"b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first DynamicFrame, see the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "dyf_retrieve_a = SelectFromCollection.apply(dyf_splitFields, \"a\")\n",
    "dyf_retrieve_a.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second Dynamic Frame, see the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "dyf_retrieve_b = SelectFromCollection.apply(dyf_splitFields, \"b\")\n",
    "dyf_retrieve_b.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SplitRows\n",
    "SplitRows allows us to filter our dataset within a specific range of counts and split them into two DynamicFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "dyf_splitRows = SplitRows.apply(frame = dyf_dropNullfields, comparison_dict = {\"`data.pineapple`\": {\">\": \"100\", \"<\": \"200\"}}, name1 = 'pa_200_less', name2 = 'pa_200_more'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first Dynamic Frame, see the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "dyf_pa_200_less = SelectFromCollection.apply(dyf_splitRows, 'pa_200_less')\n",
    "dyf_pa_200_less.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second Dynamic Frame, see the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyf_pa_200_more = SelectFromCollection.apply(dyf_splitRows, 'pa_200_more')\n",
    "dyf_pa_200_more.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spigot\n",
    "Spigot allows you to write a sample dataset to a destination during transformation. For our use case, we write the top 10 records locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "dyf_splitFields = Spigot.apply(dyf_pa_200_less, '/home/glue/GlueLocalOutput/Spigot/', 'top10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your local environment configuration, Spigot may run into errors. Alternatively, you can use an AWS Glue endpoint or an AWS Glue ETL job to run this function.\n",
    "\n",
    "## Write Dynamic Frame\n",
    "The write_dynamic_frame function writes a DynamicFrame using the specified connection and format. For our use case, we write locally (we use a connection_type of S3 with a POSIX path argument in connection_options, which allows writing to local storage):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "glueContext.write_dynamic_frame.from_options(\\\n",
    "frame = dyf_splitFields,\\\n",
    "connection_options = {'path': '/home/glue/GlueLocalOutput/'},\\\n",
    "connection_type = 's3',\\\n",
    "format = 'json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0a54084e6b208ee8d1ce3989ffc20924477a5f55f5a43e22e699a6741623861e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
