{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Filtering Using Pushdown Predicates\n",
    "In many cases, you can use a pushdown predicate to filter on partitions without having to list and read all the files in your dataset. Instead of reading the entire dataset and then filtering in a DynamicFrame, you can apply the filter directly on the partition metadata in the Data Catalog. Then you only list and read what you actually need into a DynamicFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_context.create_dynamic_frame.from_catalog(\n",
    "    database = \"my_S3_data_set\",\n",
    "    table_name = \"catalog_data_table\",\n",
    "    push_down_predicate = my_partition_predicate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Server-side filtering using catalog partition predicates\n",
    "\n",
    "he push_down_predicate option is applied after listing all the partitions from the catalog and before listing files from Amazon S3 for those partitions. If you have a lot of partitions for a table, catalog partition listing can still incur additional time overhead. To address this overhead, you can use server-side partition pruning with the catalogPartitionPredicate option that uses partition indexes in the AWS Glue Data Catalog. This makes partition filtering much faster when you have millions of partitions in one table. You can use both push_down_predicate and catalogPartitionPredicate in additional_options together if your catalogPartitionPredicate requires predicate syntax that is not yet supported with the catalog partition indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_frame = glueContext.create_dynamic_frame.from_catalog(\n",
    "    database=dbname, \n",
    "    table_name=tablename,\n",
    "    transformation_ctx=\"datasource0\",\n",
    "    push_down_predicate=\"day>=10 and customer_id like '10%'\",\n",
    "    additional_options={\"catalogPartitionPredicate\":\"year='2021' and month='06'\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "The push_down_predicate and catalogPartitionPredicate use different syntaxes. The former one uses Spark SQL standard syntax and the later one uses JSQL parser.\n",
    "\n",
    "# Writing Partitions\n",
    "By default, a DynamicFrame is not partitioned when it is written. All of the output files are written at the top level of the specified output path. Until recently, the only way to write a DynamicFrame into partitions was to convert it to a Spark SQL DataFrame before writing.\n",
    "\n",
    "However, DynamicFrames now support native partitioning using a sequence of keys, using the partitionKeys option when you create a sink. For example, the following Python code writes out a dataset to Amazon S3 in the Parquet format, into directories partitioned by the type field. From there, you can process these partitions using other systems, such as Amazon Athena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_context.write_dynamic_frame.from_options(\n",
    "    frame = projectedEvents,\n",
    "    connection_type = \"s3\",    \n",
    "    connection_options = {\"path\": \"$outpath\", \"partitionKeys\": [\"type\"]},\n",
    "    format = \"parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c69db75b480e4c127948eb459195a102407f15e89bb67469c7b13529e394f0d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
